{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fb73ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\darsh\\miniconda3\\envs\\gpu\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\darsh\\miniconda3\\envs\\gpu\\lib\\site-packages (1.26.4)\n",
      "Collecting faker\n",
      "  Downloading faker-37.4.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\darsh\\miniconda3\\envs\\gpu\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\darsh\\miniconda3\\envs\\gpu\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\darsh\\miniconda3\\envs\\gpu\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\darsh\\miniconda3\\envs\\gpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading faker-37.4.2-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/1.9 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: faker\n",
      "Successfully installed faker-37.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56699500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated CUST_MSTR_20191112.csv with 500 rows.\n",
      "✅ Generated CUST_MSTR_20191113.csv with 500 rows.\n",
      "✅ Generated CUST_MSTR_20191114.csv with 500 rows.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "fake = Faker()\n",
    "num_rows = 500\n",
    "num_files = 3\n",
    "\n",
    "start_date = datetime(2019, 11, 12)\n",
    "\n",
    "for i in range(num_files):\n",
    "    file_date = start_date + timedelta(days=i)\n",
    "    date_str = file_date.strftime('%Y%m%d')\n",
    "    join_date = file_date.strftime('%Y-%m-%d')  # consistent join date for all rows in this file\n",
    "    filename = f\"CUST_MSTR_{date_str}.csv\"\n",
    "\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Customer_ID', 'Customer_Name', 'Email', 'Join_Date'])  # Correct headers\n",
    "        for cust_id in range(1, num_rows + 1):\n",
    "            full_name = f\"{fake.first_name()} {fake.last_name()}\"\n",
    "            writer.writerow([100000 + cust_id, full_name, fake.email(), join_date])\n",
    "\n",
    "    print(f\"✅ Generated {filename} with {num_rows} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2218d009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated master_child_export-20191112.csv with 500 rows.\n",
      "Generated master_child_export-20191113.csv with 500 rows.\n",
      "Generated master_child_export-20191114.csv with 500 rows.\n"
     ]
    }
   ],
   "source": [
    "#Generate master_child_export CSV\n",
    "\n",
    "import csv\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "fake = Faker()\n",
    "num_rows = 500\n",
    "num_files = 3\n",
    "start_date = datetime(2019, 11, 12)\n",
    "\n",
    "for i in range(num_files):\n",
    "    file_date = start_date + timedelta(days=i)\n",
    "    date_str = file_date.strftime('%Y%m%d')\n",
    "    filename = f\"master_child_export-{date_str}.csv\"\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Master_ID', 'Child_ID', 'Relation_Type', 'Extracted_Date'])\n",
    "        for idx in range(1, num_rows + 1):\n",
    "            master_id = fake.random_int(1, 1000)\n",
    "            child_id = 90000 + idx\n",
    "            Realtion_Type = fake.word().capitalize()\n",
    "            Extracted_Date = file_date.strftime('%Y-%m-%d')\n",
    "            writer.writerow([master_id, child_id, Realtion_Type, Extracted_Date])\n",
    "    print(f\"Generated {filename} with {num_rows} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b05940dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated H_ECOM_ORDER_20191112.csv with 500 rows.\n",
      "✅ Generated H_ECOM_ORDER_20191113.csv with 500 rows.\n",
      "✅ Generated H_ECOM_ORDER_20191114.csv with 500 rows.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "num_rows = 500\n",
    "num_files = 3\n",
    "start_date = datetime(2019, 11, 12)\n",
    "\n",
    "for i in range(num_files):\n",
    "    file_date = start_date + timedelta(days=i)\n",
    "    date_str = file_date.strftime('%Y%m%d')\n",
    "    order_date = file_date.strftime('%Y-%m-%d')\n",
    "    filename = f\"H_ECOM_ORDER_{date_str}.csv\"\n",
    "\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Order_ID', 'Product_Name', 'Quantity', 'Order_Date', 'Extracted_Date'])  # ✅ Correct headers\n",
    "\n",
    "        for idx in range(1, num_rows + 1):\n",
    "            order_id = 30000 + idx\n",
    "            product_name = fake.first_name()\n",
    "            quantity = round(random.uniform(20.0, 100.0), 2)\n",
    "            upload_date = order_date  # ✅ From filename\n",
    "            writer.writerow([order_id, product_name, quantity, order_date, upload_date])\n",
    "\n",
    "    print(f\"✅ Generated {filename} with {num_rows} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "402554d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MasterID', 'ChildID', 'ChildName']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\darsh\\Desktop\\Main Documents Folder\\Celbal Data Engineering Summer Internship\\Week-7\\Week-7\\Data_Set\\data_csv\\master_child_export-20191114.csv\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ef98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
